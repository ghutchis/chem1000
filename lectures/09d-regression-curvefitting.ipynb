{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEM 1000 - Spring 2026\n",
    "Prof. Geoffrey Hutchison, University of Pittsburgh\n",
    "\n",
    "## 9 Linear and Nonlinear Regression\n",
    "\n",
    "These lectures notes on probability and statistics will include substantial material not found in our text.\n",
    "\n",
    "By the end of this session, you should be able to:\n",
    "- Understand nonlinear regression through *transforms*\n",
    "- Understand \"robust regression\" (to ignore outliers)\n",
    "\n",
    "We'll cover a few examples in recitation as well, particularly `curve_fit` and getting the standard errors in our fit parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear (and Nonlinear) Regression\n",
    "\n",
    "Probably all of you have fit a trendline through a set of data in Excel or another program:\n",
    "\n",
    "$$\n",
    "y = mx + b; R^2 = ...\n",
    "$$\n",
    "\n",
    "There are assumptions behind linear regression:\n",
    "\n",
    "- **Linearity**: the relationship between X and Y is linear.\n",
    "- **Independence**: observations are independent of each other (e.g., it's not a time series)\n",
    "- **Homoscedasticity**: the residual errors do not depend on X\n",
    "- **Normality**: the residual errors are normally distributed\n",
    "\n",
    "It's fairly easy to use some diagnostic plots to test these assumptions, particularly:\n",
    "- looking at the observed Y values versus the fitted values\n",
    "- looking at the residual errors as a function of X values\n",
    "\n",
    "We'll talk about the assumptions and what to do if they don't hold up later.\n",
    "\n",
    "There are several packages in Python that can (and will) do curve fitting - each has somewhat different features:\n",
    "- statsmodels\n",
    "- scipy\n",
    "- scikit-learn\n",
    "\n",
    "We're going to use `scipy.stats.linregress()` for linear regression and `scipy.optimize.curve_fit()` for general curve fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot it!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('./chem1000.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tips Data\n",
    "\n",
    "This is a somewhat famous data set:\n",
    "Bryant, P. G. and Smith, M (1995) *Practical Data Analysis: Case Studies in Business Statistics.* Homewood, IL: Richard D. Irwin Publishing\n",
    "\n",
    "Each tip recorded at a restaurant over 244 bills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to read spreadsheets like CSV\n",
    "\n",
    "tips_data = pd.read_csv('../data/tips.csv')\n",
    "tips_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we see the total bill in \\\\$ and the tip in \\\\$ as well, along with who paid the bills, whether anyone smoked (it's back from when you could smoke in restaurants), mealtime, and size of the table.\n",
    "\n",
    "Let's see how the tip scales with the total bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tips_data.total_bill, tips_data.tip)\n",
    "plt.xlabel('Total Bill ($)')\n",
    "plt.ylabel('Tip ($)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classic case of heteroscedastic errors - much larger variation with big bills than with small bills because people generally pay as a percentage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn is a Python package on top of matplotlib - it makes some plots much easier\n",
    "#    https://seaborn.pydata.org/examples/index.html\n",
    "import seaborn as sns\n",
    "\n",
    "# Draw a nested boxplot to show tips by day\n",
    "sns.boxplot(x=\"day\", y=\"tip\", hue=\"smoker\", data=tips_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the outliers in the box plots. Not sure how much data there is, but the median tip from a smoker on Sunday seems higher...\n",
    "\n",
    "The catch, though, is that we *know* tips are related to total bill. Most people tip around 15%.\n",
    "\n",
    "So let's *transform* the data and see if we can minimize the heteroscedastic errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't need to do this, but it saves me some typing\n",
    "tips = tips_data.tip\n",
    "total_bill = tips_data.total_bill\n",
    "\n",
    "percent = tips / total_bill\n",
    "\n",
    "plt.scatter(total_bill, percent * 100)\n",
    "plt.xlabel('Total Bill ($)')\n",
    "plt.ylabel('Tip (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah, that's a nice tip! Let's analyze the cases where the tip is under 30% (i.e., the usual cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = tips_data[percent < 0.3]\n",
    "\n",
    "percent = clean['tip']*100 / clean['total_bill']\n",
    "print(percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(clean['total_bill'], percent)\n",
    "plt.xlabel('Total Bill ($)')\n",
    "plt.ylabel('Tip (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's safe to say that smaller restaurant bills get higher tips.. and some people are really, really bad tippers in this data set. I mean 5% tip? That's horrible!\n",
    "\n",
    "It's still not linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(clean['total_bill'], np.sqrt(percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can feel free to play with the data more... transforming the X or Y values to get a better fit.\n",
    "\n",
    "Sometimes in science we **know** the functional form. In that case, either transform the data or use `curve_fit` with an exact function.\n",
    "\n",
    "Sometimes we don't know the functional form. So we want a fairly good empirical fit.\n",
    "\n",
    "This is sometimes known as the ladder of transforms (e.g., things to try):\n",
    "\n",
    "* Exponential $e^x$\n",
    "* Square / Cube $x^2$ or $x^3$, etc.\n",
    "* **Original Data**\n",
    "* Square Root $\\sqrt{x}$\n",
    "* Logarithm $\\log(x)$\n",
    "* Reciprocal Root $1/\\sqrt{x}$\n",
    "* Reciprocal $1/x$\n",
    "* etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression\n",
    "\n",
    "Sometimes data needs to be fit from more than one variable. Consider the fuel efficiency of a car. It depends on the mass of the car, the volume of the engine, the type of engine (V8, V6, etc.), …\n",
    "\n",
    "Let's consider the tip data further. After all, there's a lot of scatter to the data. Should we consider the size of the party? What if small tables with a big bill tip better (e.g., a date)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(clean['size'], percent)\n",
    "plt.xlabel('Table Size (people)')\n",
    "plt.ylabel('Tip (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. Seems like there are some *very* cheap 2-person tables .. maybe there's an overall effect? It's hard to know since this is probably correlated with the total bill too.\n",
    "\n",
    "We can use the `statsmodels` Python package for multi-variate fits.\n",
    "\n",
    "Particularly the `statsmodels.formula.api` or `smf` component lets us use a syntax:\n",
    "\n",
    "`percent ~ total_bill + size`\n",
    "\n",
    "This is an `ols` or Ordinary Least Squares model:\n",
    "* Intercept\n",
    "* Coefficient * total_bill\n",
    "* Coefficient * size\n",
    "\n",
    "Statsmodels also gives us some nice summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You probably don't have statsmodels installed, this will do that\n",
    "import sys\n",
    "!{sys.executable} -m pip install statsmodels --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "results = smf.ols('percent ~ total_bill + size', data=clean).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a **lot** in that summary:\n",
    "\n",
    "- $R^2$ is only 0.118. Meh.\n",
    "- Adjusted $R^2$ \"adjusts\" for the degrees of freedom - subtracting for each parameter\n",
    "- Values of the parameters and also standard errors (e.g., 18.407 ± 0.841)\n",
    "- P-values\n",
    "- Skew and kurtosis\n",
    "- etc. (there's a lot .. you don't need to understand it all)\n",
    "\n",
    "We can see that the total bill is a *negative* influence on the tip, but party size *might* be a slightly positive counter-effect. (Although if you look at the data, we can see that this place didn't have a minimum tip for large groups .. there's a 6-person table with 10\\% tip...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation - Entropy\n",
    "\n",
    "When you have enough data, it's important to *cross-validate* your model. There are a few ways to do this. The most common is to split your data into a set for training the model and another set for validation / testing.\n",
    "\n",
    "For example, maybe the model you fit only works for certain kinds of molecules?\n",
    "\n",
    "Here's some example data from a recent paper [\"Understanding conformational entropy in small molecules\"](https://doi.org/10.1021/acs.jctc.0c01213) - we're trying to understand how the entropy of a molecule depends on multiple conformations. For example, we suspect it might depend on the number of rotatable bonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training our model\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "# test data\n",
    "test_df = pd.read_csv(\"../data/holdout.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_df['NumRotors'], train_df['ConfEntropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. Maybe it connects? (Yes) Maybe we should plot other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_df['NumMethyl'], train_df['ConfEntropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the number of methyl groups *definitely* connects with our conformational entropy. But probably not linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('ConfEntropy ~ NumMethyl', data=train_df).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('ConfEntropy ~ np.log(NumMethyl+1)', data=train_df).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our model increased to an $R^2 = 0.608$ with $\\log(\\mathrm{NumMethyl} + 1)$.\n",
    "\n",
    "Let's try adding in $\\log(\\mathrm{NumRotors} + 1)$ too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = smf.ols('ConfEntropy ~ np.log(NumMethyl+1) + np.log(NumRotors+1)', data=train_df).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = 9.49 + 13.13*np.log(train_df['NumMethyl'] + 1), + 7.255*np.log(train_df['NumRotors']+1)\n",
    "plt.scatter(fitted[:1][0], train_df['ConfEntropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We can see that `statsmodels` can handle both transforming our data as well as multiple regression.\n",
    "\n",
    "We should *really* plot the residuals to check our assumptions, but I leave that as an exercise.\n",
    "\n",
    "(As it turns out the residuals aren't great .. so we did more work in the paper .. the main contributions are still from these two effects.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Regression\n",
    "\n",
    "One of the challenges with regression is that it can be strongly affected by outlier points, much like the mean average.\n",
    "\n",
    "We're going to create some artificial data with outliers to show this more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from the statsmodel website - don't worry.. just creating some artificial data with an outlier\n",
    "\n",
    "# y = 5 + 0.5x\n",
    "\n",
    "nsample = 50\n",
    "x1 = np.linspace(0, 20, nsample) # from 0 to 20 with 50 points\n",
    "X = x1\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Now we create our y values\n",
    "beta = [5, 0.5]\n",
    "y_true2 = np.dot(X, beta)  # y = 5 + 0.5x\n",
    "\n",
    "# Add some random noise\n",
    "sig = 0.3  # smaller error variance makes OLS<->RLM contrast bigger\n",
    "y2 = y_true2 + sig * 1.0 * np.random.normal(size=nsample)\n",
    "\n",
    "# Now add some outliers\n",
    "y2[[39, 41, 43, 45, 48]] -= 5  # add some outliers (10% of nsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's try to fit a quadratic function to this data.\n",
    "\n",
    "OLS = ordinary least squares (i.e., normal regression)\n",
    "RLM = robust linear model (i.e., ignore outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sm.OLS(y2, X).fit()\n",
    "print(res.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's $y = 5.589 + 0.391x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resrlm = sm.RLM(y2, X).fit()\n",
    "print(resrlm.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're much closer to the real function $y = 5.07 + 0.487x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x1, y2, \"o\", label=\"data\")\n",
    "plt.plot(x1, y_true2, \"b-\", label=\"True\")\n",
    "\n",
    "plt.plot(x1, res.fittedvalues, \"r-\", label=\"OLS\")\n",
    "plt.plot(x1, resrlm.fittedvalues, \"g.-\", label=\"RLM\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the outliers \"pull\" the linear regression to a lower slope and higher intercept, but the robust regression method ignores them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "This notebook is from Prof. Geoffrey Hutchison, University of Pittsburgh\n",
    "https://github.com/ghutchis/chem1000\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
