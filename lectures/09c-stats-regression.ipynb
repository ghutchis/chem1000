{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEM 1000 - Spring 2024\n",
    "Prof. Geoffrey Hutchison, University of Pittsburgh\n",
    "\n",
    "## 9 Statistics: Testing and Regression\n",
    "\n",
    "These lectures notes on probability and statistics will include substantial material not found in our text.\n",
    "\n",
    "By the end of this session, you should be able to:\n",
    "- Understand hypothesis testing and p-values\n",
    "    - Know about t-tests and f-tests (ANOVA)\n",
    "- Know a little bit about statistical design of experiments\n",
    "- Understand some key concepts about linear and nonlinear regression / curve-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "As a reminder, we've talked about statistics from distributions, e.g., what is the mean or median of some data.\n",
    "\n",
    "We also briefly talked about setting up tests, e.g., for COVID or drug testing, etc.\n",
    "- you might have false positives (e.g., the test shows positive, but it's wrong)\n",
    "- you might have false negatives (e.g., the test shows negative, but it's wrong)\n",
    "\n",
    "Let's go into a bit more detail.\n",
    "\n",
    "Imagine we're doing a blood test, and the standard concentration range is 3-5 ppm.\n",
    "\n",
    "<img src=\"../images/normal-testing.png\" width=\"350\" />\n",
    "\n",
    "Now, imagine we have two samples.\n",
    "- one sample has a concentration of 4.7 ppm.\n",
    "- one sample has a concentration of 7.8 ppm.\n",
    "\n",
    "The \"null hypothesis\" is that a sample concentration happened entirely by chance / normal variation.\n",
    "\n",
    "Clearly the first sample seems likely to have occurred by normal variation. It's a bit high, but there's lots of data in that part of the curve.\n",
    "\n",
    "On the other hand, the second sample seems to be *very* high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-Values and T-Tests\n",
    "\n",
    "What we want to decide is whether our new measurements happen by chance:\n",
    "- default (null) hypothesis - the new data comes from the original normal distribution\n",
    "- the alternative hypothesis - this new data is different\n",
    "\n",
    "In short, we want to know the probability of the new data coming from the original distribution.\n",
    "\n",
    "This is known as the \"p-value\"\n",
    "\n",
    "P values are the probability that a sample will have an effect at least as extreme, if the null hypothesis is correct. They're sometimes called significance levels (e.g., \"the effect is statistically significant.\")\n",
    "\n",
    "<img src=\"../images/normal-t-test2.png\" width=\"350\" />\n",
    "<img src=\"../images/normal-t-test3.png\" width=\"350\" />\n",
    "\n",
    "To compare distributions with different means, we can calculate the t-test value:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{x}_{1}-\\bar{x}_{2}-\\Delta}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}}}\n",
    "$$\n",
    "\n",
    "where $\\Delta$ is the hypothesized difference between the two samples (i.e., usually zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t = (5.0 - 4.0 - 0.0) / np.sqrt(0.5/500 + 0.5/500)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so that's the $2\\sigma$ difference... How do I interpret the t-test value?\n",
    "\n",
    "You want to look up the value in a t-test table - you'll need to know the number of degrees of freedom. That's the number of items minus one (since the mean is known).\n",
    "\n",
    "For example:\n",
    "https://www.itl.nist.gov/div898/handbook/eda/section3/eda3672.htm\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccccrrr}\n",
    "\\boldsymbol{v} & 0.90 & 0.95 & 0.975 & 0.99 & 0.995 & 0.999 \\\\\n",
    "\\hline\n",
    "1 . & 3.078 & 6.314 & 12.706 & 31.821 & 63.657 & 318.313 \\\\\n",
    "2 . & 1.886 & 2.920 & 4.303 & 6.965 & 9.925 & 22.327 \\\\\n",
    "3 . & 1.638 & 2.353 & 3.182 & 4.541 & 5.841 & 10.215 \\\\\n",
    "4 . & 1.533 & 2.132 & 2.776 & 3.747 & 4.604 & 7.173 \\\\\n",
    "5 . & 1.476 & 2.015 & 2.571 & 3.365 & 4.032 & 5.893 \\\\\n",
    "6 . & 1.440 & 1.943 & 2.447 & 3.143 & 3.707 & 5.208 \\\\\n",
    "7 . & 1.415 & 1.895 & 2.365 & 2.998 & 3.499 & 4.782 \\\\\n",
    "8 . & 1.397 & 1.860 & 2.306 & 2.896 & 3.355 & 4.499 \\\\\n",
    "9 . & 1.383 & 1.833 & 2.262 & 2.821 & 3.250 & 4.296 \\\\\n",
    "10 . & 1.372 & 1.812 & 2.228 & 2.764 & 3.169 & 4.143\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "A useful exercise is to play with some of the numbers above .. I've picked 500 items, so my t-test has great sensitivity. It's much worse if you have fewer items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (5.0 - 4.0 - 0.0) / np.sqrt(0.5/5 + 0.5/5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for 5 items, we have 4 degrees of freedom, so the $2\\sigma$ difference shows up between a p-value of 0.05 and 0.025.\n",
    "\n",
    "In other words, that difference has less than a 5% chance of occurring randomly - somewhere around 3-3.5%.\n",
    "\n",
    "Most people would call that statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Multiple Groups: ANOVA\n",
    "\n",
    "If we have to compare multiple different categories, need a different method - we don't want to run repeated t-tests.\n",
    "\n",
    "The method is called [ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance) - **AN**alysis **O**f **Va**riance. Not surprisingly, the idea is to compare the *mean* of a group and the *variance*\n",
    "\n",
    "Most often, people do a one-way ANOVA - one category with several different entries. It's possible in most software to set up a two-way (or N-way) ANOVA if you have more than one category. (Consider a study where you're testing pH, temperature, and two different catalysts.)\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "The default (null) hypothesis is that all the groups are the same - they have the same mean.\n",
    "\n",
    "That means we're trying to show at least one group has a different mean.\n",
    "\n",
    "We often use a [box plot](https://en.wikipedia.org/wiki/Box_plot) to show these trends:\n",
    "\n",
    "<img src=\"../images/anova1.png\" width=\"350\" />\n",
    "\n",
    "<img src=\"../images/anova2.png\" width=\"350\" />\n",
    "\n",
    "Notice in the second, the effect is more pronounced.\n",
    "\n",
    "Much like the t-test, we generate a \"p-value\" for whether any group is different from the rest by chance.\n",
    "\n",
    "(For example, it might be statistically significant difference with 0.05 chance, 0.01 chance, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Design of Experiments\n",
    "\n",
    "It's a whole topic or course, but we can use t-tests and ANOVA to consider how to set up experiments. For example, we can compare 3-4 different things at once (e.g., three different pH or four different temperatures).\n",
    "\n",
    "We can also use the statistical significance and expectations of the size of the effect to estimate the number of data points (e.g., how many people do we want in our COVID vaccine trial to be sure there's a difference in infection rate.. and that it's similar across men, women, different ethnic groups, etc.)\n",
    "\n",
    "Of course a statistically significant effect may or may not be *practically* important.\n",
    "\n",
    "I can take lots of data points and show that my software calculates integrals 1% faster. Is that practically important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear (and Nonlinear) Regression\n",
    "\n",
    "Probably all of you have fit a trendline through a set of data in Excel or another program:\n",
    "\n",
    "$$\n",
    "y = mx + b; R^2 = ...\n",
    "$$\n",
    "\n",
    "There are assumptions behind linear regression:\n",
    "\n",
    "- **Linearity**: the relationship between X and Y is linear.\n",
    "- **Independence**: observations are independent of each other (e.g., it's not a time series)\n",
    "- **Homoscedasticity**: the residual errors do not depend on X\n",
    "- **Normality**: the residual errors are normally distributed\n",
    "\n",
    "It's fairly easy to use some diagnostic plots to test these assumptions, particularly:\n",
    "- looking at the observed Y values versus the fitted values\n",
    "- looking at the residual errors as a function of X values\n",
    "\n",
    "We'll talk about the assumptions and what to do if they don't hold up later.\n",
    "\n",
    "There are several packages in Python that can (and will) do curve fitting - each has somewhat different features:\n",
    "- statsmodels\n",
    "- scipy\n",
    "- scikit-learn\n",
    "\n",
    "We're going to use `scipy.stats.linregress()` for linear regression and `scipy.optimize()` for general curve fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot it!\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('./chem1000.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create toy data \n",
    "x = np.arange(0, 10, 0.2) # remember this will create an array from 0 to 10 with 0.25 intervals\n",
    "y = x + (np.random.rand(len(x)) * 6) # just add some random scatter to the x-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "import scipy.stats\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "print(slope, intercept, r_value**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, slope*x + intercept, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so by definition, that's the best line that fits the data. How confident are we in the result? Well, we know the slope should be 1.0, but the intercept will be random, so we don't know that for sure.\n",
    "\n",
    "There are formulas for confidence intervals, but before we get to that, we can actually see how those formulas work.\n",
    "\n",
    "We're going to **bootstrap**. It's a really simple numerical method that works for a lot of statistics problem.\n",
    "- randomly take a subset of the data (say 50%, 75%, etc.)\n",
    "- perform our analysis (e.g., perform a regression, take the mean/average, etc.)\n",
    "- repeat a lot\n",
    "- look at the average and variance of the resulting repeated samples\n",
    "\n",
    "An example might be useful: consider if I want to know if a coin flip is fair. If I flip it (i.e., sample randomly) 1000 times, I should have pretty reliable data due to the law of large numbers.\n",
    "\n",
    "In this case, let's sample from our data, perform a linear regression and repeat a lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://stats.stackexchange.com/a/218988/42723\n",
    "\n",
    "# create a plot for all our bootstrap regresssions\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# how many times to do the bootstrap sampling\n",
    "n_boot = 1000\n",
    "a = 3.0 / n_boot # show some transparency - if you change the figsize, you'll want to tweak this\n",
    "for i in range(0, n_boot):\n",
    "    # randomly pick from the data\n",
    "    sample_index = np.random.choice(range(0, len(y)), len(y))\n",
    "\n",
    "    x_samples = x[sample_index]\n",
    "    y_samples = y[sample_index]    \n",
    "\n",
    "    # perform the linear regresssion on our random samples\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x_samples, y_samples)\n",
    "    # we'll plot after applying the function to the whole range\n",
    "    plt.plot(x, slope*x + intercept, color='grey', alpha=a)\n",
    "\n",
    "# scatter plot for the original data\n",
    "plt.scatter(x,y, marker='o', color='blue')\n",
    "# plot our best-fit line\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "plt.plot(x, slope*x + intercept, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there's a range of intercepts and slopes. These roughly outline **confidence intervals** on our linear regression.\n",
    "- The center region will have tighter confidence intervals\n",
    "- Edges will have broader ranges due to possible variation in the intercept and slope\n",
    "\n",
    "Diagnostics?\n",
    "\n",
    "Let's be careful and check if our regression satisfies the four assumptions:\n",
    "- Linearity - looks okay, but we'll check that\n",
    "- Independence - well, we constructed it to be independent, but we'll check that too\n",
    "- Homoscedasticity - we'll need to look at the residuals\n",
    "- Normality - ditto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = slope*x + intercept\n",
    "residuals = fitted - y # my original \"observations\"\n",
    "\n",
    "plt.scatter(x, residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, looks fairly random...\n",
    "- it's *not* heteroscedastic (we'll see an example in a bit)\n",
    "- *probably* normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption checklist:\n",
    "- Linearity - looks good - if it's nonlinear the residuals show patterns (hang on)\n",
    "- Independence - same thing - not usually a problem with a one-variable fit\n",
    "- Homoscedasticity - looks good from plotting the residuals\n",
    "- Normality - the histogram looks.. eh? not great, not bad.. maybe?\n",
    "\n",
    "In principal, you should do this for all regressions.\n",
    "\n",
    "You *definitely* need to do this if it's a borderline case..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Regression\n",
    "\n",
    "Sometimes your data doesn't follow a linear function. Even if it looks mostly linear.\n",
    "\n",
    "From [XKCD](https://xkcd.com/2048/):\n",
    "<img src=\"https://imgs.xkcd.com/comics/curve_fitting_2x.png\" width=\"413\" />\n",
    "\n",
    "A recent research project in the group is testing several methods of calculating the molecular polarizability $\\alpha$. Polarizability is connected to the induced dipole moment, dielectric constants, index of refraction... etc.\n",
    "\n",
    "We're comparing two methods:\n",
    "- GFN2 is an approximate method.. but it's really fast (e.g., a few seconds per compound)\n",
    "- $\\omega$B97X is a density functional method.. much more accurate, but minutes to hours per compound\n",
    "\n",
    "Here's a subset of the smaller molecules in the test set:\n",
    "\n",
    "<img src=\"../images/polarizabilities.png\" width=\"350\" />\n",
    "\n",
    "So that's a slope of 1.00, an intercept of 0.00, and a really nice $R^2$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we had a bigger spreadsheet, I might use the 'pandas' module\n",
    "alpha_gfn2 = np.loadtxt('../data/polarizabilities.csv', skiprows=1, usecols=[0], delimiter=',')\n",
    "alpha_wb97 = np.loadtxt('../data/polarizabilities.csv', skiprows=1, usecols=[1], delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(alpha_gfn2, alpha_wb97)\n",
    "print(slope, intercept, r_value**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that's not the same... Slope is now ~1.5, and there's an intercept of -10.8?\n",
    "\n",
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot it\n",
    "\n",
    "# plot our best-fit line\n",
    "plt.plot(alpha_gfn2, slope*alpha_gfn2 + intercept, color='red')\n",
    "\n",
    "# raw data\n",
    "plt.scatter(alpha_gfn2, alpha_wb97, alpha=0.15)\n",
    "# axis labels - might look a bit confusing to get the Greek letters\n",
    "plt.ylabel(r'$\\omega$ B97X $\\alpha$ (Å$^3$)')\n",
    "plt.xlabel(r'GFN2 $\\alpha$ (Å$^3$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even ignoring the obvious outliers, this doesn't look so good.\n",
    "- The intercept should go through the origin\n",
    "- The errors are small for small values, but much bigger as you go to the right\n",
    "     - Uh oh, that's *heteroscedasticity*!\n",
    "- If the fit is 1.00 for small values, why is it 1.5 now?\n",
    "    - Is this data truly linear?\n",
    "    \n",
    "Let's plot the diagnostic plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted = slope * alpha_gfn2 + intercept\n",
    "residuals = alpha_wb97 - fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(residuals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(alpha_gfn2, residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh. That doesn't look good\n",
    "- the residuals probably aren't normal .. although it's skewed by the huge outlier\n",
    "- the heteroscedasticity shows clearly - errors get big towards the right\n",
    "- there seems to be a slope to negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms and Nonlinear Curve Fitting\n",
    "\n",
    "There are a few solutions when the data isn't linear. If you've used Excel, you've probably changed the fit function, for example:\n",
    "- quadratic ($a + bx + cx^2$)\n",
    "- exponential ($ae^x$)\n",
    "- power ($a*x^b$)\n",
    "\n",
    "You can make this explicit by transforming x, y, or both:\n",
    "- take the square, cube, or higher power\n",
    "- take the square root or logarithm\n",
    "- take the reciprocal, $1/x^2$, etc.\n",
    "\n",
    "I'm going to show a more general route - you can specify whatever function you want. In that case, you should be a little careful, since it's possible to fit an [elephant with four parameters](https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/).\n",
    "\n",
    "Let's try to fit this to a quadratic function with zero intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fit function - notice, no intercept, so that's forced through zero\n",
    "def fit(x, b, c):\n",
    "    y = b*x + c*x**2\n",
    "    return y\n",
    "\n",
    "parameters, covariance = scipy.optimize.curve_fit(fit, alpha_gfn2, alpha_wb97)\n",
    "# this prints all the parameters in the array\n",
    "print(*parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the fit function is:\n",
    "\n",
    "$$\n",
    "y = 0.9367x + 0.005576x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = parameters[0]\n",
    "c = parameters[1]\n",
    "\n",
    "# to plot this, we want to create a set of x-values\n",
    "x_values = np.arange(0, 130, 1)\n",
    "fitted_values = fit(x_values, b, c)\n",
    "\n",
    "plt.scatter(alpha_gfn2, alpha_wb97, alpha=0.15)\n",
    "plt.plot(x_values, fitted_values, '-', label='fit', color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sure *looks* better.\n",
    "\n",
    "We can also calculate the standard error of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also calculate the standard error of the parameters\n",
    "SE = np.sqrt(np.diag(covariance))\n",
    "SE_b = SE[0]\n",
    "SE_c = SE[1]\n",
    "\n",
    "print('slope:', round(b, 5), 'standard error', round(SE_b, 5))\n",
    "print('quadratic:', round(c, 5), 'standard error', round(SE_c, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in addition to the $R^2$ value, it's often useful to calculate the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = fitted - alpha_wb97 # same as the residuals\n",
    "absolute_errors = np.abs(errors)\n",
    "\n",
    "print(\"MAE: \", np.mean(absolute_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "This notebook is from Prof. Geoffrey Hutchison, University of Pittsburgh\n",
    "https://github.com/ghutchis/chem1000\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
