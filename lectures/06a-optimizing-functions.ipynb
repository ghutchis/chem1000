{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEM 1000 - Fall 2020\n",
    "Prof. Geoffrey Hutchison, University of Pittsburgh\n",
    "\n",
    "## 6 Optimizing Functions\n",
    "\n",
    "Chapter 6 in [*Mathematical Methods for Chemists*](http://sites.bu.edu/straub/mathematical-methods-for-molecular-science/)\n",
    "\n",
    "By the end of this session, you should be able to:\n",
    "- Understand general approaches to optimize functions either for minima or maxima (i.e. \"extrema\")\n",
    "- Understand how to use derivatives in multiple dimensions to categorize extrema\n",
    "- Using `scipy.optimize` to do numeric optimization of complicated functions\n",
    "  - (We'll have more examples for both algebra/calculus and numerical optimization in recitation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why\n",
    "\n",
    "In chemistry and physics, we often want to determine the maximum or minimum value of a function of one or many variables. Examples include characterizing the minima, maxima, and saddle points on a potential energy surface.\n",
    "\n",
    "Remember from Calculus, that at a maximum or a minimum, the derivative must be zero.\n",
    "\n",
    "<img src=\"../images/extrema.png\" />\n",
    "\n",
    "To decide on the type of extrema, we look at the derivative:\n",
    "\n",
    "$$\n",
    "\\left.\\frac{d^{2} f(x)}{d x^{2}}\\right|_{x=x^{*}}=\\left\\{\\begin{array}{ll}\n",
    "<0, & \\text { maximum } \\\\\n",
    "=0, & \\text { inflection point } \\\\\n",
    ">0, & \\text { minimum }\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "In other words, the second derivative is negative at a maxima because it \"points down.\" At a minima, the second derivative is positive and \"points up.\" At an inflection point, the derivative isn't zero, but the second derivative is, because on one side it is positive and on the other side it is negative.\n",
    "\n",
    "Thus, optimizing functions in one dimension is pretty easy, if sometimes tedious.\n",
    "1. Take derivatives and find where the first derivative is zero\n",
    "2. Look at the second derivatives, to categorize as a minima / maxima / inflection point\n",
    "3. Then compare values of the function at those points to see if it's a local minima / max or the global minima / max.\n",
    "\n",
    "### Many Variables\n",
    "\n",
    "Not surprisingly, we can use a similar technique in multiple dimensions.\n",
    "\n",
    "If we have a function $f(x,y)$ in two dimensions, then to have an extrema:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}=0 \\quad \\frac{\\partial f}{\\partial y}=0\n",
    "$$\n",
    "\n",
    "In other words, we need to see the partial derivative with respect to **both / all** variables be zero.\n",
    "\n",
    "We can then categorize the type of minima / maxima with the [Hessian]. (Later, we will see that this is the *determinant* of the Hessian matrix, for when we have more than 2 variables.)\n",
    "\n",
    "$$\n",
    "D=\\left(\\frac{\\partial^{2} f}{\\partial x^{2}}\\right)\\left(\\frac{\\partial^{2} f}{\\partial y^{2}}\\right)-\\left(\\frac{\\partial^{2} f}{\\partial x \\partial y}\\right)\\left(\\frac{\\partial^{2} f}{\\partial y \\partial x}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\left.\\frac{\\partial^{2} f}{\\partial x^{2}}\\right|_{\\left(x^{*}, y^{*}\\right)}=\\left\\{\\begin{array}{ll}\n",
    "<0 & D>0 & \\text { maximum } \\\\\n",
    ">0 & D>0 & \\text { minimum } \\\\\n",
    " & D < 0 & \\text { saddle-point }\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's try the example of a potential energy surface (i.e., a double-well potential):\n",
    "\n",
    "$$\n",
    "V(x, y)=\\left(x^{2}-1\\right)^{2}+y^{2}\n",
    "$$\n",
    "\n",
    "<img src='../images/saddle.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import init_session\n",
    "init_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = (x**2 - 1)**2 + y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, let's see where the partial derivative with x is zero...\n",
    "diff(V, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the Sympy solve() function\n",
    "# although that one isn't very hard\n",
    "solve( diff(V, x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do y\n",
    "diff(V, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so for this potential energy function, we have three zero points:\n",
    "- (-1, 0)\n",
    "- (0, 0)\n",
    "- (+1, 0)\n",
    "\n",
    "What kind of points are these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's the two-variable Hessian test...\n",
    "D = diff(V, x, 2)*diff(V, y, 2) - diff(V, x, y)*diff(V, y, x)\n",
    "D # print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the .subs() method to substitute values (-1, 0)\n",
    "D.subs([ (x, -1), (y, 0) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check (0,0)\n",
    "D.subs([ (x, 0), (y, 0) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check (+1, 0)\n",
    "D.subs([ (x, 1), (y, 0) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap:\n",
    "- (-1, 0): D is positive, second derivative is positive, this is a minima\n",
    "- (0, 0): D is negative, this must be a saddle point\n",
    "- (+1, 0): D is positive, second derivative is positive, this is a minima\n",
    "\n",
    "So we can use the Hessian D to establish the type of minima / maxima / saddle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Complicated Functions\n",
    "\n",
    "Sometimes in chemistry, functions are complicated enough that we can't easily find all the zeros.\n",
    "\n",
    "When?\n",
    "\n",
    "Let's imagine I want to optimize the geometry of a molecule. Each atom can move independently of the other atoms, so that's:\n",
    "- X, Y, and Z displacement for atom 1\n",
    "- X, Y, and Z displacement for atom 2\n",
    "- .. etc.\n",
    "\n",
    "So for N atoms, that's 3 displacements per atom, or 3N. To be precise, I'll need to subtract out cases where I move all the atoms the same amount in the X, Y, or Z directions - that's just moving the whole molecule. And I'll need to subtract our cases where I rotate the whole molecule along the X, Y, or Z axis.. so generally I have 3N-6 variabels to optimize. For water, that's 3 variables, but even for ethane, that's 8 atoms and 18 variables to optimize at once.\n",
    "\n",
    "How do we optimize ethane or something more complicated?\n",
    "\n",
    "<img src=\"../images/atom-forces.png\" width=\"341\" />\n",
    "\n",
    "Since we can presumably calculate the potential energy as a function of the atom positions, we calculate:\n",
    "\n",
    "$$\n",
    "V(1, 2, 3 .. 8)\n",
    "$$\n",
    "\n",
    "Then we calculate the gradients on each atom:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{F} = -\\boldsymbol{\\nabla} V\n",
    "$$\n",
    "\n",
    "We push the atoms a little bit, and repeat. How much do we push the atoms? There are a few methods:\n",
    "- [steepest gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "- [conjugate gradient](https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
    "- [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) or limited-memory L-BFGS\n",
    "\n",
    "In some sense these are all methods that determine how much you should move along the gradient, and then what should you do .. to eventually end up at the extrema. (You pick whether you're going uphill or downhill to Point State Park.)\n",
    "\n",
    "### Example: \n",
    "\n",
    "A standard case for optimizing multi-variate functions is the [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function):\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\sum_{i=1}^{N-1} 100\\left(x_{i+1}-x_{i}^{2}\\right)^{2}+\\left(1-x_{i}\\right)^{2}\n",
    "$$\n",
    "\n",
    "So you pick some number of dimensions N, and the minimum is clearly when all the $x_i = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# the scipy.optimize module has the Rosenbrock function\n",
    "from scipy.optimize import rosen\n",
    "# it also has a bunch of minimize methods\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# here are our initial values - 5 dimensions\n",
    "# all somewhat close to 1\n",
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "\n",
    "rosen(x0) # close to 1, but still pretty big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can minimize using the Conjugate Gradients method \"CG\"\n",
    "# the 'disp' True part will show us some information about how many steps\n",
    "optima = minimize(rosen, x0, method='CG', options={'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable result has a bunch of things in it\n",
    "dir(optima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most useful is 'x' - the array of minimized values\n",
    "print(optima.x)\n",
    "# also useful is 'fun' - the value of the function at optima.x\n",
    "# e.g., rosen(optima.x)\n",
    "print(optima.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, we can use the Conjugate Gradients method to optimize the function - it took 67 steps and 834 evaluations of the Rosenbrock function. Are there better algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try BFGS instead of \"CG\"\n",
    "optima = minimize(rosen, x0, method='BFGS', options={'disp': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the optimized x values and rosen(optima.x)\n",
    "print(optima.x)\n",
    "print(optima.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the BFGS method takes fewer steps then conjugate gradients (25 vs. 67) and fewer function evaluations (180 vs. 834).\n",
    "\n",
    "If we had a more complicated function, a slower computer, .. maybe we're doing quantum mechanics and calculating the energy of the molecule takes time.. clearly we want to use efficient optimization methods. BFGS and the L-BFGS method are pretty good if you have can easily calculate the gradient.\n",
    "\n",
    "Now if you can also easily calculate the Hessian, you can use another method - a modified Newton's method \"Newton-CG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we import the known derivative and Hessian of the Rosenbrock function\n",
    "# - the Gradient (rosen_der)\n",
    "# - the Hessian (rosen_hess)\n",
    "from scipy.optimize import rosen_der, rosen_hess\n",
    "\n",
    "# Newton-CG is a little more complicated\n",
    "# because we have to give these too\n",
    "optima = minimize(rosen, x0, method='Newton-CG',\n",
    "               jac=rosen_der, hess=rosen_hess,\n",
    "               options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize:\n",
    "- Conjugate Gradients (only need gradient, 67 steps, 834 function calls)\n",
    "- BFGS (also only need gradient, 25 steps, 180 function calls)\n",
    "- Newton-CG (gradient and Hessian, 21 steps, 30 function calls)\n",
    "\n",
    "In short, when we have really complicated functions, we optimize numerically using one of these methods, ideally BFGS or a Newton-CG method depending on  whether we have the gradient and/or Hessian to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond these techniques, there is a whole area of mathematics on \"[optimization theory](https://en.wikipedia.org/wiki/Mathematical_optimization)\"\n",
    "\n",
    "Gradient-free techniques include:\n",
    "- [Particle swarm optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization)\n",
    "- [Simultaneous perturbation stochastic approximation](https://www.jhuapl.edu/SPSA/)\n",
    "- [Genetic algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm) - also work with discrete or non-continuous functions\n",
    "- [Simulated annealing](https://en.wikipedia.org/wiki/Simulated_annealing)\n",
    "- [Simplex / Nelder-Mead](https://en.wikipedia.org/wiki/Nelderâ€“Mead_method)\n",
    "\n",
    "Many of these gradient-free techniques involve estimating a [*surrogate function*](https://en.wikipedia.org/wiki/Surrogate_model) -- a simpler system to be optimized, e.g. [Bayesian optimization / Gaussian processs regression](https://en.wikipedia.org/wiki/Bayesian_optimization). (Some texts call this [\"kriging\"](https://en.wikipedia.org/wiki/Kriging).)\n",
    "\n",
    "**Multiple Objectives**\n",
    "\n",
    "Some cases, you may want to optimize more than one property that have a trade-off. This is known as [Pareto optimization](https://en.wikipedia.org/wiki/Pareto_efficiency), for example fuel efficiency and engine horsepower in a car. \n",
    "\n",
    "In quantum chemistry, there is often a trade-off between accuracy and speed. You may make approximations that result in a faster but less accurate method (e.g., you approximate or ignore certain time-consuming integrals). Or you may want a higher accuracy calculation, but that means it will take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "This notebook is from Prof. Geoffrey Hutchison, University of Pittsburgh\n",
    "https://github.com/ghutchis/chem1000\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
