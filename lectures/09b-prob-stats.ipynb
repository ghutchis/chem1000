{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHEM 1000 - Spring 2025\n",
    "Prof. Geoffrey Hutchison, University of Pittsburgh\n",
    "\n",
    "## 9 From Probability to Statistics\n",
    "\n",
    "These lectures notes on probability and statistics will include substantial material not found in our text.\n",
    "\n",
    "By the end of this session, you should be able to:\n",
    "- Understand mean, median, mode, standard deviation, and standard error\n",
    "    - Know when you might want mean or median\n",
    "    - Know the difference between standard deviation and standard error of the mean\n",
    "- Know a little bit about false positives and false negatives thanks to Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics from Probability Distributions\n",
    "\n",
    "As a reminder, last time we talked about probability distributions and important properties:\n",
    "\n",
    "These are related to the shape of the distribution.\n",
    "- the total (e.g., for a probability it should be 1 = 100%)\n",
    "- the [mean](https://en.wikipedia.org/wiki/Expected_value) $\\mu$ (i.e., the center or \"expected value\")\n",
    "- the [variance](https://en.wikipedia.org/wiki/Variance) $\\sigma^2$ (i.e., the width)\n",
    "    - you're probably more familiar with the standard deviation $\\sigma$\n",
    "- the [skewness](https://en.wikipedia.org/wiki/Skewness) (i.e., the asymmetry of the distribution)\n",
    "- the [kurtosis](https://en.wikipedia.org/wiki/Kurtosis) (i.e., how thin or thick the \"tail\" of the distribution)\n",
    "\n",
    "For normal Gaussian distributions, there are useful rules about how much data can be found within certain intervals (e.g., $\\pm 2\\sigma$) around the mean.\n",
    "\n",
    "### Real (Finite) Data\n",
    "\n",
    "There are a few very important points.\n",
    "\n",
    "1. For real data, we almost never know the **true** distribution, much less the true mean, true standard deviation, etc.\n",
    "    - Well, to paraphrase my college statistics professor, unless you hear a deep voice \"Your data is truly from a Gaussian distribution with mean 0.5 and standard deviation 0.01...\"\n",
    "2. What we know instead, are a finite approximation called the \"sampling distribution\".\n",
    "3. We indicate this using regular letters. So if the true mean is $\\mu$ and standard deviation is $\\sigma$ we'll use $\\bar{x}$ and $s$ respectively - these are approximations to the true values.\n",
    "\n",
    "### Non-Normal Distributions, Medians, and Robust Stats\n",
    "\n",
    "Consider some bond lengths - these are gathered from the [Crystallographic Open Database](https://www.crystallography.net):\n",
    "\n",
    "<img src=\"../images/C2n2.png\" width=\"300\" />\n",
    "\n",
    "Notice that this probability distribution has some asymmetry to the distribution. Do you think the mean comes at the exact peak?\n",
    "\n",
    "<img src=\"../images/C2N2-mean.png\" width=\"300\" />\n",
    "\n",
    "A few other \"central measures\" are useful for cases like this:\n",
    "\n",
    "- The *mode* is the most frequent value. In this case, you'd average the two central columns and get 1.345 Å - shorter than the average.\n",
    "\n",
    "- The *median* is the middle value. You sort all the values, and find the one in the middle. If it's an even number you average them.\n",
    "\n",
    "- The *trimmed mean* - ignore the top and bottom 5% and calculate the mean of the rest.\n",
    "    - Other trimming values can be used, especially if you have a lot of data (e.g., toss the top 10% and bottom 10%, take the mean of the remaining 80%).\n",
    "\n",
    "For this data, both the median and the mode are shorter (to the right) of the mean because the data is skewed to longer lengths.\n",
    "\n",
    "These are examples of so-called \"robust statistics\" - that is, statistics which are less affected by outlier points or skew.\n",
    "\n",
    "Another good example is with net worth. What if Bill Gates or Jeff Bezos decided to sit in to audit our class. (Hi Bill!) If we take the mean of net worth, it goes way, way up. Clearly that's not a very representative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we can load in the c-n bond length data set ~8000 bonds\n",
    "cn_lengths = np.loadtxt('../data/c2n2-bonds.csv', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean', np.mean(cn_lengths))\n",
    "print('std dev.', np.std(cn_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors and Variance\n",
    "\n",
    "Why would anyone use the mean if it's distorted by an outlier or skew in the data?\n",
    "\n",
    "Let's think about three possible data sets:\n",
    "- A set of 8 bond lengths\n",
    "- My 8,000 bond length example\n",
    "- A set of 8,000,000 C-N bond lengths across millions of molecules\n",
    "\n",
    "Which one do you think will have more accurate statistics?\n",
    "\n",
    "It's something of a trick -- clearly it's possible to have lots of bad data, but usually having more data improves accuracy of predictions.\n",
    "\n",
    "This leads us to the concept of the [**standard error**](https://en.wikipedia.org/wiki/Standard_error):\n",
    "\n",
    "(From Wikipedia)\n",
    ">Put simply, the standard error of the sample mean is an estimate of how far the sample mean is likely to be from the population mean, whereas the standard deviation of the sample is the degree to which individuals within the sample differ from the sample mean\n",
    "\n",
    "In other words, our intuition reflects the standard error - the more data we gather, the smaller the error bars around the mean of the data.\n",
    "\n",
    "Generally, we use the estimate:\n",
    "\n",
    "$$\n",
    "\\sigma_{\\bar{x}} \\approx \\frac{s}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "where \"n\" is the number of points. If we want to cut the error bars in half, we need 4x as many data points.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In science, it's important to understand the distinction between the standard error and standard deviation (and when to use one or the other).\n",
    "    \n",
    "</div>\n",
    "\n",
    "But what about the median? Isn't there a standard error of the median?\n",
    "\n",
    "Yes, but it's not always trivial to calculate - usually you've picked the median because the data isn't from a Gaussian distribution, so the error bars on the median aren't clear.\n",
    "\n",
    "Instead, you can use a trick called \"bootstrapping.\" Basically, you repeatedly random chunks of your data, calculate the median of that chunk.. and then you can see how much those medians vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from statistics import mean, median\n",
    "\n",
    "# bootstrap confidence intervals\n",
    "# repeatedly resample the data into random subsets\n",
    "# .. take the mean and medians of the subset\n",
    "def boot_sample(list, iterations=1000, fraction=0.75):\n",
    "    size = int( len(list) * fraction) # how big is each subset\n",
    "    means = []\n",
    "    medians = []\n",
    "    for i in range(iterations):\n",
    "        subset = resample(list, n_samples=size) # grab a random chunk\n",
    "        means.append(mean(subset)) # calculate the mean of the subset\n",
    "        medians.append(median(subset)) # calc. the median\n",
    "    return means, medians # return sets of means and medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means, medians = boot_sample(cn_lengths, iterations=500)\n",
    "\n",
    "med = np.median(cn_lengths)\n",
    "low_cir = med - np.percentile(medians, 2.5)\n",
    "high_cir = np.percentile(medians, 97.5) - med\n",
    "print(low_cir, med, high_cir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we might say that the median is 1.3531 ± 0.0007 Å, which seems pretty good.\n",
    "\n",
    "What about the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stderr = np.std(cn_lengths) / np.sqrt(len(cn_lengths))\n",
    "print(stderr, np.mean(cn_lengths), stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we might choose the mean because the standard error of the mean is much smaller than error bars around the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Probability and Statistics\n",
    "\n",
    "[Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes) became very interested in probability. (Ironically, the portrait of him is probably not.)\n",
    "\n",
    "In particular, Bayes was interested in conditional probability and inverse probability.\n",
    "\n",
    "<img src=\"../images/bayesian.png\" width=250 />\n",
    "\n",
    "Let's imagine we put in 3 blue balls and 4 red balls into the basket.\n",
    "\n",
    "What's the probability we pull out a blue ball?\n",
    "- Obviously 3 out of 7\n",
    "\n",
    "If I make 10 observations, what can I infer about the distribution?\n",
    "- What if you pull out 10 blue balls?\n",
    "- What if you pull out 10 red balls?\n",
    "\n",
    "Put another way, if you're gambling, at what point do you decide a coin flip is no longer fair?\n",
    "\n",
    "Here's [Bayes's Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)\n",
    "\n",
    "$$\n",
    "P(A \\mid B)=\\frac{P(B \\mid A) P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "* $P(A\\mid B)$ is a conditional probability: the likelihood of event A occurring given that B is true.\n",
    "* $P(B\\mid A)$ is also a conditional probability: the likelihood of event B occurring given that A is true.\n",
    "* $P(A)$ and $P(B)$ are the probabilities of observing A and B.\n",
    "\n",
    "Why is this useful?\n",
    "\n",
    "Imagine we want to do some sort of drug test (e.g., similar for COVID, cancer, etc.)\n",
    "\n",
    "- the test is 90% *sensitive* - it gives a positive result 90% of the time when someone is positive. (For COVID, you'd probably want much higher sensitivity.) That means there's a 10% **false negative** rate.\n",
    "- the test is 80% *specific* - it gives 80% true negative rate - which means 20% of people get a **false positive**. (This gets into ethics, but presumably it's better to err on a false positive than a false negative, e.g. COVID)\n",
    "\n",
    "Let's say hypothetically there's 5% *prevalence* of drug use in a population.\n",
    "\n",
    "What's the chance that someone with a positive drug test is actually a drug user?\n",
    "\n",
    "$$\n",
    "P(\\text { User } \\mid \\text { Positive })=\\frac{P(\\text { Positive } \\mid \\text { User }) P(\\text { User })}{P(\\text { Positive })}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text { User } \\mid \\text { Positive }) = \\frac{0.90 \\times 0.05}{0.90 \\times 0.05+0.20 \\times 0.95} = 0.191\n",
    "$$\n",
    "\n",
    "In other words, most positive test results are false positives because prevalence is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find better specificity\n",
    "for false_positive in [0.2, 0.1, 0.05, 0.01]:\n",
    "    print('rate:', 0.9*0.05 / (0.9*.05 + false_positive*0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, Bayes's theorem tells us that if prevalence is low, we want really good specificity - i.e. very few false positives.\n",
    "\n",
    "Of course it depends on the consequences:\n",
    "- for a drug test, presumably you don't want to accuse someone who's innocent.\n",
    "- for COVID, presumably you really, really don't want a false negative - they can spread it\n",
    "    - a false positive is presumably not as bad..\n",
    "    \n",
    "### Final Words about Bayes\n",
    "\n",
    "Another way to look at Bayes and so-called \"Bayesian Statistics\" is that it's a way to look at your hypothesis:\n",
    "\n",
    "<img src=\"../images/bayes-hypothesis.png\" width=\"300\" />\n",
    "\n",
    "In other words, Bayes's theorem gives us the likelihood our hypothesis is true, given the evidence.\n",
    "\n",
    "Isn't that what we want in science?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "This notebook is from Prof. Geoffrey Hutchison, University of Pittsburgh\n",
    "https://github.com/ghutchis/chem1000\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
